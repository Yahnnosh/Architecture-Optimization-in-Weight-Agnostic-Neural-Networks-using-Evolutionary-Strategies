{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    'tanh': tf.tanh,\n",
    "    'relu': tf.nn.relu,\n",
    "    'sigmoid': tf.nn.sigmoid,\n",
    "    'linear': tf.keras.activations.linear,\n",
    "    'softmax': tf.nn.softmax,\n",
    "    'sign': tf.sign,\n",
    "    'sin': tf.sin,\n",
    "    'exp': tf.exp\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Loading Data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "SUBSET = 1.0    # subset (in percentage) of X_test used during training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 250 ms\n",
      "Wall time: 256 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# numpy\n",
    "_, (X_test, y_test) = mnist.load_data() # only care  about X_test\n",
    "\n",
    "selection = np.random.choice(np.arange(X_test.shape[0]),\n",
    "                             int(SUBSET * X_test.shape[0]),\n",
    "                             replace=False)\n",
    "\n",
    "X_test = X_test.reshape(10000, 784).astype(np.float32)[selection] / 255.0   # flatten\n",
    "y_test = to_categorical(y_test)[selection]  # one-hot encoding\n",
    "\n",
    "# tensorflow\n",
    "X_test = tf.convert_to_tensor(np.transpose(X_test))\n",
    "y_test = tf.convert_to_tensor(np.transpose(y_test))\n",
    "\n",
    "# for evaluation\n",
    "y_true = np.argmax(y_test, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Network Definition**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "MUTATE_RATE_MATRIX = 0.2\n",
    "MUTATE_RATE_BIAS = 0.2\n",
    "MUTATE_RATE_ACTIVATION_FUNCTION = 0.2\n",
    "CROSSOVER_RATE = 0.5\n",
    "GAUSSIAN_NOISE_STDDEV = 1   # mutation applies additive gaussian noise\n",
    "UNIFORM_CROSSOVER = False   # if True, performs crossover of matrices element-wise, else row-wise\n",
    "HIDDEN_LAYER_WIDTH = 32     # TODO: for now all hidden layers same width"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(Model):\n",
    "    def __init__(self, **params):\n",
    "        \"\"\"\n",
    "        Weight gnostic multi-layer feed forward neural network\n",
    "        :param params: Params have to be in form: (matrix1=..., bias1=..., activations1=..., matrix2=..., ...)\n",
    "        \"\"\"\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.n_layers = max([int(param_name[-1]) for param_name in params.keys()])   # = number of hidden layers + 1 (output layer)\n",
    "\n",
    "        for (param_name, param) in params.items():\n",
    "            assert param_name[:-1] in ('matrix', 'bias', 'activation'), 'Invalid attribute!'\n",
    "            setattr(self, param_name, param)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        for layer in range(1, self.n_layers + 1):\n",
    "            x = getattr(self, 'matrix' + str(layer)) @ x\n",
    "            x += getattr(self, 'bias' + str(layer))\n",
    "            x = activation_functions[getattr(self, 'activation' + str(layer))](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = np.argmax(self.call(X_test), axis=0)\n",
    "        return np.mean(y_pred == y_true)\n",
    "\n",
    "    def mutate(self):\n",
    "        for layer in range(1, self.n_layers + 1):\n",
    "            # matrix\n",
    "            matrix = getattr(self, 'matrix' + str(layer))\n",
    "            mutation_stencil = tf.cast(tf.reshape(tf.random.categorical(\n",
    "                tf.math.log([[1 - MUTATE_RATE_MATRIX, MUTATE_RATE_MATRIX]]),\n",
    "                matrix.shape[0] * matrix.shape[1]), matrix.shape), tf.float32)\n",
    "            noise = tf.random.normal(mean=0.0, stddev=GAUSSIAN_NOISE_STDDEV, shape=matrix.shape)\n",
    "            matrix = matrix + tf.multiply(mutation_stencil, noise)\n",
    "            setattr(self, 'matrix' + str(layer), matrix)\n",
    "\n",
    "            # bias\n",
    "            bias = getattr(self, 'bias' + str(layer))\n",
    "            mutation_stencil = tf.cast(tf.reshape(tf.random.categorical(\n",
    "                tf.math.log([[1 - MUTATE_RATE_BIAS, MUTATE_RATE_BIAS]]),\n",
    "                bias.shape[0]), bias.shape), tf.float32)\n",
    "            noise = tf.random.normal(mean=0.0, stddev=GAUSSIAN_NOISE_STDDEV, shape=bias.shape)\n",
    "            bias = bias + tf.multiply(mutation_stencil, noise)\n",
    "            setattr(self, 'bias' + str(layer), bias)\n",
    "\n",
    "            # activation\n",
    "            cleaner = lambda x: 'softmax' if x=='softmax_v2' else x\n",
    "            activation = cleaner(getattr(self, 'activation' + str(layer)))\n",
    "            if random.uniform(0, 1) < MUTATE_RATE_ACTIVATION_FUNCTION:\n",
    "                activation = random.choice(list(activation_functions.keys()))\n",
    "            setattr(self, 'activation' + str(layer), activation)\n",
    "\n",
    "    def summary(self):\n",
    "        dash = '-' * 75\n",
    "        ddash = '=' * 75\n",
    "        print(dash)\n",
    "        print('Model')\n",
    "        print(ddash)\n",
    "\n",
    "        n_params = 0\n",
    "        for layer in range(1, self.n_layers + 1):\n",
    "            # get values\n",
    "            matrix = getattr(self, 'matrix' + str(layer))\n",
    "            bias = getattr(self, 'bias' + str(layer))\n",
    "            cleaner = lambda x: 'softmax' if x=='softmax_v2' else x\n",
    "            activation = cleaner(getattr(self, 'activation' + str(layer)))\n",
    "\n",
    "            n_params += matrix.shape[0] * matrix.shape[1] + bias.shape[0] + 1\n",
    "\n",
    "            # print adjustments\n",
    "            activation = '({})'.format(activation)\n",
    "            layer_IO = '(in={}, out={})'.format(matrix.shape[1], matrix.shape[0],)\n",
    "\n",
    "            print('Linear {:<20}{:<30}#Params: {}'.format(activation, layer_IO, matrix.shape[0] * matrix.shape[1] + bias.shape[0] + 1))\n",
    "\n",
    "        print(ddash)\n",
    "        print('Total params: {}'.format(n_params))\n",
    "        print('Accuracy: {}\\n'.format(round(self.evaluate(), 3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "class Population:\n",
    "    # TODO: n_layers evolvable\n",
    "    def __init__(self, size=10, n_survivors=5, n_hidden_layers=1):\n",
    "        \"\"\"\n",
    "        :param size: population size\n",
    "        :param n_survivors: number of survivors after each generation (rest is killed and unable to pass on its genes)\n",
    "        :param n_hidden_layers: number of hidden layers\n",
    "        \"\"\"\n",
    "        self.generation = 0\n",
    "        self.size = size\n",
    "        self.n_survivors = n_survivors\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.elite = None\n",
    "        self.fitness = None # cache fitness for increased speed\n",
    "        self.fitness_generation = -1  # generation when fitness was evaluated\n",
    "\n",
    "        # initialization (gaussian)\n",
    "        # TODO: hidden layer width fixed for now\n",
    "        self.organisms = []\n",
    "        for _ in range(size):\n",
    "            params = {}\n",
    "\n",
    "            n_neurons_prev = 784\n",
    "            n_neurons_curr = HIDDEN_LAYER_WIDTH\n",
    "            for layer in range(1, self.n_hidden_layers + 2):\n",
    "                if layer == self.n_hidden_layers + 1:\n",
    "                    n_neurons_curr = 10  # output layer\n",
    "                params['matrix' + str(layer)] = tf.random.normal(mean=0.0, stddev=1.0, shape=[n_neurons_curr, n_neurons_prev])\n",
    "                params['bias' + str(layer)] = tf.random.normal(mean=0.0, stddev=1.0, shape=[n_neurons_curr, 1])\n",
    "                params['activation' + str(layer)] = 'sigmoid'\n",
    "                n_neurons_prev = HIDDEN_LAYER_WIDTH\n",
    "\n",
    "            model = MultiLayerPerceptron(**params)\n",
    "            self.organisms.append(model)\n",
    "\n",
    "        self.history = [(max(self.organism_fitness()), self.average_fitness())]   # fitness of population over all generations\n",
    "\n",
    "    def organism_fitness(self):\n",
    "        if self.generation != self.fitness_generation:\n",
    "            self.fitness = [organism.evaluate() for organism in self.organisms]\n",
    "            self.fitness_generation = self.generation\n",
    "\n",
    "        return self.fitness\n",
    "\n",
    "    def average_fitness(self):\n",
    "        organism_fitness = self.organism_fitness()\n",
    "        return sum(organism_fitness) / len(organism_fitness)\n",
    "\n",
    "    def max_fitness(self):\n",
    "        return max(self.organism_fitness())\n",
    "\n",
    "    def selection(self):\n",
    "        organism_fitness = self.organism_fitness()\n",
    "\n",
    "        # elitism (n=1)\n",
    "        elite_index = np.argmax(organism_fitness)\n",
    "        self.elite = self.organisms.pop(elite_index)\n",
    "        organism_fitness.pop(elite_index)\n",
    "\n",
    "        probabilities = [fitness / sum(organism_fitness) for fitness in organism_fitness]  # normalized\n",
    "        survivors = np.random.choice(self.organisms,\n",
    "                                     size=self.n_survivors - 1,\n",
    "                                     p=probabilities,\n",
    "                                     replace=False)\n",
    "        return [survivor for survivor in survivors]\n",
    "\n",
    "    def crossover(self, parents):\n",
    "        children = []\n",
    "        while len(children) < int(CROSSOVER_RATE * (self.size - 1)):\n",
    "            [father, mother] = random.sample(parents + [self.elite], k=2)  # sample without replacement\n",
    "\n",
    "            child_params = {}\n",
    "            for layer in range(1, father.n_layers + 1):\n",
    "                if UNIFORM_CROSSOVER:\n",
    "                    # matrix - uniform crossover\n",
    "                    father_matrix = getattr(father, 'matrix' + str(layer))\n",
    "                    mother_matrix = getattr(mother, 'matrix' + str(layer))\n",
    "\n",
    "                    father_mask = tf.round(tf.random.uniform(father_matrix.shape))\n",
    "                    mother_mask = - (father_mask - 1)\n",
    "\n",
    "                    child_matrix = tf.multiply(father_mask, father_matrix) + tf.multiply(mother_mask, mother_matrix)\n",
    "                    child_params['matrix' + str(layer)] = child_matrix\n",
    "                else:\n",
    "                    # matrix - row-wise (neuron-wise) crossover\n",
    "                    father_matrix = getattr(father, 'matrix' + str(layer))\n",
    "                    mother_matrix = getattr(mother, 'matrix' + str(layer))\n",
    "\n",
    "                    n_rows = father_matrix.shape[0]\n",
    "                    father_mask = np.random.choice([True, False], size=n_rows)\n",
    "\n",
    "                    child_matrix = tf.convert_to_tensor([father_matrix[row, :] if father_mask[row] \\\n",
    "                                                         else mother_matrix[row, :] for row in range(n_rows)])\n",
    "                    child_params['matrix' + str(layer)] = child_matrix\n",
    "\n",
    "                # bias - uniform crossover\n",
    "                father_bias = getattr(father, 'bias' + str(layer))\n",
    "                mother_bias = getattr(mother, 'bias' + str(layer))\n",
    "\n",
    "                father_mask = tf.round(tf.random.uniform(father_bias.shape))\n",
    "                mother_mask = - (father_mask - 1)\n",
    "\n",
    "                child_bias = tf.multiply(father_mask, father_bias) + tf.multiply(mother_mask, mother_bias)\n",
    "                child_params['bias' + str(layer)] = child_bias\n",
    "\n",
    "                # activation\n",
    "                cleaner = lambda x: 'softmax' if x=='softmax_v2' else x\n",
    "                father_activation = cleaner(getattr(father, 'activation' + str(layer)))\n",
    "                mother_activation = cleaner(getattr(mother, 'activation' + str(layer)))\n",
    "\n",
    "                child_activation = father_activation if (random.uniform(0, 1) < 0.5) else mother_activation\n",
    "                child_params['activation' + str(layer)] = child_activation\n",
    "\n",
    "            model = MultiLayerPerceptron(**child_params)\n",
    "            children.append(model)\n",
    "\n",
    "        # if CROSSOVER_RATE != 100% allow some individuals to pass on their genes without crossover\n",
    "        while len(children) < (self.size - 1):\n",
    "            [model] = random.sample(parents + [self.elite], k=1)  # sample without replacement\n",
    "\n",
    "            child_params = {}\n",
    "            for layer in range(1, model.n_layers + 1):\n",
    "                # matrix\n",
    "                child_params['matrix' + str(layer)] = getattr(model, 'matrix' + str(layer))\n",
    "\n",
    "                # bias\n",
    "                child_params['bias' + str(layer)] = getattr(model, 'bias' + str(layer))\n",
    "\n",
    "                # activation\n",
    "                cleaner = lambda x: 'softmax' if x=='softmax_v2' else x\n",
    "                child_params['activation' + str(layer)] = cleaner(getattr(model, 'activation' + str(layer)))\n",
    "\n",
    "            model = MultiLayerPerceptron(**child_params)\n",
    "            children.append(model)\n",
    "\n",
    "        return children\n",
    "\n",
    "    def mutate(self, organisms):\n",
    "        for organism in organisms:\n",
    "            organism.mutate()\n",
    "\n",
    "    def breed(self, debug=False):\n",
    "        if debug:\n",
    "            time_debug = ''\n",
    "\n",
    "            t_a = time.time()\n",
    "            parents = self.selection()  # ~0.0005s\n",
    "            t_b = time.time()\n",
    "            time_debug += 'selection time: {}s - '.format(round(t_b - t_a, 4))\n",
    "\n",
    "            t_a = time.time()\n",
    "            children = self.crossover(parents)  # ~0.28s\n",
    "            t_b = time.time()\n",
    "            time_debug += 'crossover time: {}s - '.format(round(t_b - t_a, 4))\n",
    "\n",
    "            t_a = time.time()\n",
    "            self.mutate(children)  # ~0.15s#\n",
    "            t_b = time.time()\n",
    "            time_debug += 'mutation time: {}s - '.format(round(t_b - t_a, 4))\n",
    "\n",
    "            print(time_debug)\n",
    "        else:\n",
    "            parents = self.selection()\n",
    "            children = self.crossover(parents)\n",
    "            self.mutate(children)\n",
    "\n",
    "        self.organisms = children + [self.elite]\n",
    "        self.generation += 1\n",
    "        self.history.append((self.max_fitness(), self.average_fitness()))\n",
    "\n",
    "    def plot(self):\n",
    "        # plot evolution\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(self.generation + 1), [score[0] for score in self.history],\n",
    "                 label='max fitness')\n",
    "        plt.plot(np.arange(self.generation + 1), [score[1] for score in self.history],\n",
    "                 label='avg fitness', alpha=0.6)\n",
    "        plt.title('Population fitness' + ' (n=' + str(self.size) + ')')\n",
    "        plt.xlabel('Generations')\n",
    "        plt.ylabel('Fitness score (accuracy)')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # plot best performing final network\n",
    "        organism_fitness = self.organism_fitness()\n",
    "        elite_index = np.argmax(organism_fitness)\n",
    "        self.organisms[elite_index].summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "# initialization\n",
    "GENERATIONS = 2000\n",
    "POPULATION_SIZE = 20\n",
    "SURVIVORS = 10\n",
    "N_HIDDEN_LAYERS = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Gen 0 :   avg: 0.110  -  max: 0.162 (0.08s)\n",
      "Gen 1 :   avg: 0.114  -  max: 0.174 (0.21s)\n",
      "Gen 2 :   avg: 0.119  -  max: 0.176 (0.27s)\n",
      "Gen 3 :   avg: 0.114  -  max: 0.176 (0.21s)\n",
      "Gen 4 :   avg: 0.117  -  max: 0.184 (0.21s)\n",
      "Gen 5 :   avg: 0.125  -  max: 0.184 (0.23s)\n",
      "Gen 6 :   avg: 0.118  -  max: 0.184 (0.22s)\n",
      "Gen 7 :   avg: 0.138  -  max: 0.191 (0.21s)\n",
      "Gen 8 :   avg: 0.130  -  max: 0.193 (0.21s)\n",
      "Gen 9 :   avg: 0.126  -  max: 0.193 (0.21s)\n",
      "Gen 10 :   avg: 0.131  -  max: 0.193 (0.27s)\n",
      "Gen 11 :   avg: 0.125  -  max: 0.193 (0.23s)\n",
      "Gen 12 :   avg: 0.127  -  max: 0.193 (0.21s)\n",
      "Gen 13 :   avg: 0.138  -  max: 0.193 (0.22s)\n",
      "Gen 14 :   avg: 0.144  -  max: 0.232 (0.21s)\n",
      "Gen 15 :   avg: 0.145  -  max: 0.232 (0.21s)\n",
      "Gen 16 :   avg: 0.138  -  max: 0.232 (0.25s)\n",
      "Gen 17 :   avg: 0.140  -  max: 0.232 (0.21s)\n",
      "Gen 18 :   avg: 0.139  -  max: 0.232 (0.21s)\n",
      "Gen 19 :   avg: 0.151  -  max: 0.232 (0.2s)\n",
      "Gen 20 :   avg: 0.151  -  max: 0.232 (0.24s)\n",
      "Gen 21 :   avg: 0.140  -  max: 0.232 (0.22s)\n",
      "Gen 22 :   avg: 0.137  -  max: 0.232 (0.22s)\n",
      "Gen 23 :   avg: 0.136  -  max: 0.232 (0.24s)\n",
      "Gen 24 :   avg: 0.122  -  max: 0.232 (0.24s)\n",
      "Gen 25 :   avg: 0.136  -  max: 0.232 (0.24s)\n",
      "Gen 26 :   avg: 0.127  -  max: 0.232 (0.24s)\n",
      "Gen 27 :   avg: 0.135  -  max: 0.232 (0.22s)\n",
      "Gen 28 :   avg: 0.137  -  max: 0.232 (0.23s)\n",
      "Gen 29 :   avg: 0.136  -  max: 0.232 (0.23s)\n",
      "Gen 30 :   avg: 0.139  -  max: 0.232 (0.21s)\n",
      "Gen 31 :   avg: 0.134  -  max: 0.232 (0.22s)\n",
      "Gen 32 :   avg: 0.141  -  max: 0.232 (0.24s)\n",
      "Gen 33 :   avg: 0.158  -  max: 0.241 (0.22s)\n",
      "Gen 34 :   avg: 0.148  -  max: 0.250 (0.24s)\n",
      "Gen 35 :   avg: 0.132  -  max: 0.250 (0.24s)\n",
      "Gen 36 :   avg: 0.128  -  max: 0.250 (0.23s)\n",
      "Gen 37 :   avg: 0.156  -  max: 0.250 (0.24s)\n",
      "Gen 38 :   avg: 0.147  -  max: 0.256 (0.24s)\n",
      "Gen 39 :   avg: 0.162  -  max: 0.256 (0.24s)\n",
      "Gen 40 :   avg: 0.153  -  max: 0.256 (0.24s)\n",
      "Gen 41 :   avg: 0.152  -  max: 0.256 (0.24s)\n",
      "Gen 42 :   avg: 0.148  -  max: 0.256 (0.24s)\n",
      "Gen 43 :   avg: 0.173  -  max: 0.256 (0.24s)\n",
      "Gen 44 :   avg: 0.169  -  max: 0.256 (0.24s)\n",
      "Gen 45 :   avg: 0.172  -  max: 0.256 (0.24s)\n",
      "Gen 46 :   avg: 0.164  -  max: 0.256 (0.23s)\n",
      "Gen 47 :   avg: 0.156  -  max: 0.256 (0.23s)\n",
      "Gen 48 :   avg: 0.162  -  max: 0.256 (0.23s)\n",
      "Gen 49 :   avg: 0.166  -  max: 0.256 (0.25s)\n",
      "Gen 50 :   avg: 0.170  -  max: 0.256 (0.26s)\n",
      "Gen 51 :   avg: 0.158  -  max: 0.256 (0.27s)\n",
      "Gen 52 :   avg: 0.164  -  max: 0.275 (0.24s)\n",
      "Gen 53 :   avg: 0.174  -  max: 0.275 (0.24s)\n",
      "Gen 54 :   avg: 0.173  -  max: 0.275 (0.25s)\n",
      "Gen 55 :   avg: 0.160  -  max: 0.275 (0.24s)\n",
      "Gen 56 :   avg: 0.181  -  max: 0.275 (0.25s)\n",
      "Gen 57 :   avg: 0.174  -  max: 0.275 (0.25s)\n",
      "Gen 58 :   avg: 0.160  -  max: 0.275 (0.24s)\n",
      "Gen 59 :   avg: 0.160  -  max: 0.275 (0.24s)\n",
      "Gen 60 :   avg: 0.166  -  max: 0.275 (0.24s)\n",
      "Gen 61 :   avg: 0.163  -  max: 0.275 (0.23s)\n",
      "Gen 62 :   avg: 0.149  -  max: 0.275 (0.23s)\n",
      "Gen 63 :   avg: 0.144  -  max: 0.275 (0.23s)\n",
      "Gen 64 :   avg: 0.168  -  max: 0.275 (0.23s)\n",
      "Gen 65 :   avg: 0.165  -  max: 0.275 (0.25s)\n",
      "Gen 66 :   avg: 0.151  -  max: 0.275 (0.24s)\n",
      "Gen 67 :   avg: 0.163  -  max: 0.275 (0.24s)\n",
      "Gen 68 :   avg: 0.159  -  max: 0.275 (0.23s)\n",
      "Gen 69 :   avg: 0.145  -  max: 0.275 (0.24s)\n",
      "Gen 70 :   avg: 0.135  -  max: 0.275 (0.24s)\n",
      "Gen 71 :   avg: 0.162  -  max: 0.275 (0.24s)\n",
      "Gen 72 :   avg: 0.170  -  max: 0.275 (0.24s)\n",
      "Gen 73 :   avg: 0.168  -  max: 0.275 (0.24s)\n",
      "Gen 74 :   avg: 0.170  -  max: 0.275 (0.23s)\n",
      "Gen 75 :   avg: 0.171  -  max: 0.275 (0.24s)\n",
      "Gen 76 :   avg: 0.175  -  max: 0.275 (0.24s)\n",
      "Gen 77 :   avg: 0.173  -  max: 0.275 (0.24s)\n",
      "Gen 78 :   avg: 0.185  -  max: 0.275 (0.23s)\n",
      "Gen 79 :   avg: 0.187  -  max: 0.275 (0.25s)\n",
      "Gen 80 :   avg: 0.157  -  max: 0.275 (0.25s)\n",
      "Gen 81 :   avg: 0.163  -  max: 0.275 (0.26s)\n",
      "Gen 82 :   avg: 0.174  -  max: 0.275 (0.26s)\n",
      "Gen 83 :   avg: 0.195  -  max: 0.292 (0.26s)\n",
      "Gen 84 :   avg: 0.164  -  max: 0.292 (0.25s)\n",
      "Gen 85 :   avg: 0.162  -  max: 0.292 (0.24s)\n",
      "Gen 86 :   avg: 0.166  -  max: 0.294 (0.25s)\n",
      "Gen 87 :   avg: 0.152  -  max: 0.294 (0.23s)\n",
      "Gen 88 :   avg: 0.190  -  max: 0.294 (0.24s)\n",
      "Gen 89 :   avg: 0.194  -  max: 0.294 (0.24s)\n",
      "Gen 90 :   avg: 0.197  -  max: 0.298 (0.24s)\n",
      "Gen 91 :   avg: 0.182  -  max: 0.298 (0.24s)\n",
      "Gen 92 :   avg: 0.162  -  max: 0.298 (0.25s)\n",
      "Gen 93 :   avg: 0.150  -  max: 0.298 (0.23s)\n",
      "Gen 94 :   avg: 0.154  -  max: 0.298 (0.24s)\n",
      "Gen 95 :   avg: 0.145  -  max: 0.298 (0.24s)\n",
      "Gen 96 :   avg: 0.159  -  max: 0.298 (0.23s)\n",
      "Gen 97 :   avg: 0.178  -  max: 0.298 (0.24s)\n",
      "Gen 98 :   avg: 0.191  -  max: 0.298 (0.23s)\n",
      "Gen 99 :   avg: 0.191  -  max: 0.298 (0.25s)\n",
      "Gen 100 :   avg: 0.202  -  max: 0.300 (0.24s)\n",
      "Gen 101 :   avg: 0.179  -  max: 0.300 (0.24s)\n",
      "Gen 102 :   avg: 0.191  -  max: 0.307 (0.24s)\n",
      "Gen 103 :   avg: 0.173  -  max: 0.323 (0.24s)\n",
      "Gen 104 :   avg: 0.178  -  max: 0.323 (0.24s)\n",
      "Gen 105 :   avg: 0.189  -  max: 0.323 (0.25s)\n",
      "Gen 106 :   avg: 0.199  -  max: 0.323 (0.24s)\n",
      "Gen 107 :   avg: 0.207  -  max: 0.323 (0.26s)\n",
      "Gen 108 :   avg: 0.229  -  max: 0.323 (0.26s)\n",
      "Gen 109 :   avg: 0.197  -  max: 0.323 (0.26s)\n",
      "Gen 110 :   avg: 0.207  -  max: 0.323 (0.24s)\n",
      "Gen 111 :   avg: 0.209  -  max: 0.323 (0.25s)\n",
      "Gen 112 :   avg: 0.187  -  max: 0.323 (0.26s)\n",
      "Gen 113 :   avg: 0.192  -  max: 0.323 (0.25s)\n",
      "Gen 114 :   avg: 0.198  -  max: 0.323 (0.23s)\n",
      "Gen 115 :   avg: 0.191  -  max: 0.323 (0.25s)\n",
      "Gen 116 :   avg: 0.166  -  max: 0.323 (0.23s)\n",
      "Gen 117 :   avg: 0.198  -  max: 0.323 (0.24s)\n",
      "Gen 118 :   avg: 0.187  -  max: 0.323 (0.23s)\n",
      "Gen 119 :   avg: 0.213  -  max: 0.323 (0.22s)\n",
      "Gen 120 :   avg: 0.204  -  max: 0.323 (0.21s)\n",
      "Gen 121 :   avg: 0.179  -  max: 0.323 (0.22s)\n",
      "Gen 122 :   avg: 0.200  -  max: 0.323 (0.21s)\n",
      "Gen 123 :   avg: 0.194  -  max: 0.323 (0.22s)\n",
      "Gen 124 :   avg: 0.207  -  max: 0.323 (0.22s)\n",
      "Gen 125 :   avg: 0.189  -  max: 0.323 (0.22s)\n",
      "Gen 126 :   avg: 0.200  -  max: 0.323 (0.22s)\n",
      "Gen 127 :   avg: 0.184  -  max: 0.323 (0.23s)\n",
      "Gen 128 :   avg: 0.184  -  max: 0.323 (0.23s)\n",
      "Gen 129 :   avg: 0.201  -  max: 0.323 (0.23s)\n",
      "Gen 130 :   avg: 0.195  -  max: 0.323 (0.24s)\n",
      "Gen 131 :   avg: 0.216  -  max: 0.323 (0.23s)\n",
      "Gen 132 :   avg: 0.196  -  max: 0.323 (0.23s)\n",
      "Gen 133 :   avg: 0.196  -  max: 0.323 (0.22s)\n",
      "Gen 134 :   avg: 0.196  -  max: 0.323 (0.23s)\n",
      "Gen 135 :   avg: 0.214  -  max: 0.323 (0.22s)\n",
      "Gen 136 :   avg: 0.203  -  max: 0.323 (0.22s)\n",
      "Gen 137 :   avg: 0.183  -  max: 0.323 (0.22s)\n",
      "Gen 138 :   avg: 0.187  -  max: 0.323 (0.23s)\n",
      "Gen 139 :   avg: 0.186  -  max: 0.323 (0.23s)\n",
      "Gen 140 :   avg: 0.183  -  max: 0.332 (0.23s)\n",
      "Gen 141 :   avg: 0.205  -  max: 0.332 (0.24s)\n",
      "Gen 142 :   avg: 0.231  -  max: 0.332 (0.24s)\n",
      "Gen 143 :   avg: 0.237  -  max: 0.332 (0.24s)\n",
      "Gen 144 :   avg: 0.224  -  max: 0.332 (0.24s)\n",
      "Gen 145 :   avg: 0.207  -  max: 0.346 (0.23s)\n",
      "Gen 146 :   avg: 0.228  -  max: 0.346 (0.22s)\n",
      "Gen 147 :   avg: 0.205  -  max: 0.346 (0.22s)\n",
      "Gen 148 :   avg: 0.205  -  max: 0.346 (0.22s)\n",
      "Gen 149 :   avg: 0.181  -  max: 0.346 (0.22s)\n",
      "Gen 150 :   avg: 0.169  -  max: 0.346 (0.23s)\n",
      "Gen 151 :   avg: 0.210  -  max: 0.346 (0.22s)\n",
      "Gen 152 :   avg: 0.190  -  max: 0.346 (0.23s)\n",
      "Gen 153 :   avg: 0.195  -  max: 0.346 (0.23s)\n",
      "Gen 154 :   avg: 0.196  -  max: 0.346 (0.22s)\n",
      "Gen 155 :   avg: 0.174  -  max: 0.346 (0.23s)\n",
      "Gen 156 :   avg: 0.184  -  max: 0.346 (0.23s)\n",
      "Gen 157 :   avg: 0.176  -  max: 0.346 (0.22s)\n",
      "Gen 158 :   avg: 0.212  -  max: 0.346 (0.22s)\n",
      "Gen 159 :   avg: 0.204  -  max: 0.346 (0.22s)\n",
      "Gen 160 :   avg: 0.199  -  max: 0.346 (0.21s)\n",
      "Gen 161 :   avg: 0.232  -  max: 0.346 (0.22s)\n",
      "Gen 162 :   avg: 0.246  -  max: 0.346 (0.22s)\n",
      "Gen 163 :   avg: 0.238  -  max: 0.346 (0.22s)\n",
      "Gen 164 :   avg: 0.195  -  max: 0.346 (0.22s)\n",
      "Gen 165 :   avg: 0.177  -  max: 0.346 (0.22s)\n",
      "Gen 166 :   avg: 0.175  -  max: 0.346 (0.22s)\n",
      "Gen 167 :   avg: 0.231  -  max: 0.346 (0.21s)\n",
      "Gen 168 :   avg: 0.245  -  max: 0.346 (0.21s)\n",
      "Gen 169 :   avg: 0.211  -  max: 0.349 (0.24s)\n",
      "Gen 170 :   avg: 0.193  -  max: 0.349 (0.25s)\n",
      "Gen 171 :   avg: 0.179  -  max: 0.350 (0.24s)\n",
      "Gen 172 :   avg: 0.168  -  max: 0.350 (0.22s)\n",
      "Gen 173 :   avg: 0.154  -  max: 0.350 (0.22s)\n",
      "Gen 174 :   avg: 0.135  -  max: 0.350 (0.22s)\n",
      "Gen 175 :   avg: 0.130  -  max: 0.350 (0.25s)\n",
      "Gen 176 :   avg: 0.173  -  max: 0.350 (0.23s)\n",
      "Gen 177 :   avg: 0.207  -  max: 0.350 (0.24s)\n",
      "Gen 178 :   avg: 0.233  -  max: 0.350 (0.22s)\n",
      "Gen 179 :   avg: 0.238  -  max: 0.350 (0.22s)\n",
      "Gen 180 :   avg: 0.250  -  max: 0.350 (0.22s)\n",
      "Gen 181 :   avg: 0.235  -  max: 0.356 (0.23s)\n",
      "Gen 182 :   avg: 0.222  -  max: 0.359 (0.23s)\n",
      "Gen 183 :   avg: 0.194  -  max: 0.359 (0.23s)\n",
      "Gen 184 :   avg: 0.211  -  max: 0.359 (0.22s)\n",
      "Gen 185 :   avg: 0.246  -  max: 0.359 (0.22s)\n",
      "Gen 186 :   avg: 0.229  -  max: 0.359 (0.22s)\n",
      "Gen 187 :   avg: 0.212  -  max: 0.359 (0.22s)\n",
      "Gen 188 :   avg: 0.204  -  max: 0.359 (0.22s)\n",
      "Gen 189 :   avg: 0.189  -  max: 0.359 (0.22s)\n",
      "Gen 190 :   avg: 0.170  -  max: 0.359 (0.22s)\n",
      "Gen 191 :   avg: 0.170  -  max: 0.359 (0.21s)\n",
      "Gen 192 :   avg: 0.179  -  max: 0.359 (0.22s)\n",
      "Gen 193 :   avg: 0.186  -  max: 0.359 (0.22s)\n",
      "Gen 194 :   avg: 0.204  -  max: 0.359 (0.22s)\n",
      "Gen 195 :   avg: 0.218  -  max: 0.359 (0.21s)\n",
      "Gen 196 :   avg: 0.231  -  max: 0.359 (0.22s)\n",
      "Gen 197 :   avg: 0.162  -  max: 0.359 (0.23s)\n",
      "Gen 198 :   avg: 0.167  -  max: 0.359 (0.22s)\n",
      "Gen 199 :   avg: 0.207  -  max: 0.359 (0.22s)\n",
      "Gen 200 :   avg: 0.228  -  max: 0.359 (0.22s)\n",
      "Gen 201 :   avg: 0.230  -  max: 0.359 (0.23s)\n",
      "Gen 202 :   avg: 0.243  -  max: 0.359 (0.23s)\n",
      "Gen 203 :   avg: 0.200  -  max: 0.359 (0.24s)\n",
      "Gen 204 :   avg: 0.217  -  max: 0.359 (0.24s)\n",
      "Gen 205 :   avg: 0.227  -  max: 0.359 (0.23s)\n",
      "Gen 206 :   avg: 0.252  -  max: 0.359 (0.23s)\n",
      "Gen 207 :   avg: 0.227  -  max: 0.359 (0.24s)\n",
      "Gen 208 :   avg: 0.227  -  max: 0.359 (0.23s)\n",
      "Gen 209 :   avg: 0.210  -  max: 0.359 (0.22s)\n",
      "Gen 210 :   avg: 0.200  -  max: 0.359 (0.23s)\n",
      "Gen 211 :   avg: 0.217  -  max: 0.359 (0.23s)\n",
      "Gen 212 :   avg: 0.233  -  max: 0.359 (0.22s)\n",
      "Gen 213 :   avg: 0.235  -  max: 0.359 (0.22s)\n",
      "Gen 214 :   avg: 0.200  -  max: 0.359 (0.22s)\n",
      "Gen 215 :   avg: 0.204  -  max: 0.359 (0.22s)\n",
      "Gen 216 :   avg: 0.217  -  max: 0.359 (0.22s)\n",
      "Gen 217 :   avg: 0.200  -  max: 0.359 (0.22s)\n",
      "Gen 218 :   avg: 0.214  -  max: 0.359 (0.22s)\n",
      "Gen 219 :   avg: 0.230  -  max: 0.359 (0.22s)\n",
      "Gen 220 :   avg: 0.237  -  max: 0.359 (0.23s)\n",
      "Gen 221 :   avg: 0.263  -  max: 0.359 (0.23s)\n",
      "Gen 222 :   avg: 0.231  -  max: 0.359 (0.22s)\n",
      "Gen 223 :   avg: 0.266  -  max: 0.360 (0.23s)\n",
      "Gen 224 :   avg: 0.246  -  max: 0.360 (0.24s)\n",
      "Gen 225 :   avg: 0.226  -  max: 0.360 (0.23s)\n",
      "Gen 226 :   avg: 0.238  -  max: 0.360 (0.22s)\n",
      "Gen 227 :   avg: 0.221  -  max: 0.360 (0.22s)\n",
      "Gen 228 :   avg: 0.218  -  max: 0.360 (0.22s)\n",
      "Gen 229 :   avg: 0.216  -  max: 0.360 (0.22s)\n",
      "Gen 230 :   avg: 0.214  -  max: 0.360 (0.22s)\n",
      "Gen 231 :   avg: 0.207  -  max: 0.360 (0.23s)\n",
      "Gen 232 :   avg: 0.244  -  max: 0.360 (0.24s)\n",
      "Gen 233 :   avg: 0.229  -  max: 0.363 (0.24s)\n",
      "Gen 234 :   avg: 0.237  -  max: 0.363 (0.22s)\n",
      "Gen 235 :   avg: 0.222  -  max: 0.363 (0.23s)\n",
      "Gen 236 :   avg: 0.187  -  max: 0.363 (0.22s)\n",
      "Gen 237 :   avg: 0.200  -  max: 0.363 (0.25s)\n",
      "Gen 238 :   avg: 0.206  -  max: 0.363 (0.24s)\n",
      "Gen 239 :   avg: 0.203  -  max: 0.363 (0.23s)\n",
      "Gen 240 :   avg: 0.210  -  max: 0.363 (0.22s)\n",
      "Gen 241 :   avg: 0.199  -  max: 0.375 (0.21s)\n",
      "Gen 242 :   avg: 0.225  -  max: 0.375 (0.22s)\n",
      "Gen 243 :   avg: 0.235  -  max: 0.375 (0.22s)\n",
      "Gen 244 :   avg: 0.252  -  max: 0.375 (0.38s)\n",
      "Gen 245 :   avg: 0.250  -  max: 0.375 (0.22s)\n",
      "Gen 246 :   avg: 0.273  -  max: 0.375 (0.22s)\n",
      "Gen 247 :   avg: 0.235  -  max: 0.375 (0.22s)\n",
      "Gen 248 :   avg: 0.245  -  max: 0.375 (0.22s)\n",
      "Gen 249 :   avg: 0.227  -  max: 0.375 (0.22s)\n",
      "Gen 250 :   avg: 0.271  -  max: 0.375 (0.24s)\n",
      "Gen 251 :   avg: 0.258  -  max: 0.391 (0.22s)\n",
      "Gen 252 :   avg: 0.233  -  max: 0.391 (0.23s)\n",
      "Gen 253 :   avg: 0.244  -  max: 0.391 (0.23s)\n",
      "Gen 254 :   avg: 0.223  -  max: 0.391 (0.24s)\n",
      "Gen 255 :   avg: 0.217  -  max: 0.392 (0.22s)\n",
      "Gen 256 :   avg: 0.219  -  max: 0.392 (0.22s)\n",
      "Gen 257 :   avg: 0.263  -  max: 0.392 (0.23s)\n",
      "Gen 258 :   avg: 0.272  -  max: 0.392 (0.24s)\n",
      "Gen 259 :   avg: 0.228  -  max: 0.392 (0.23s)\n",
      "Gen 260 :   avg: 0.233  -  max: 0.392 (0.23s)\n",
      "Gen 261 :   avg: 0.253  -  max: 0.395 (0.24s)\n",
      "Gen 262 :   avg: 0.238  -  max: 0.400 (0.23s)\n",
      "Gen 263 :   avg: 0.230  -  max: 0.400 (0.27s)\n",
      "Gen 264 :   avg: 0.241  -  max: 0.400 (0.24s)\n",
      "Gen 265 :   avg: 0.230  -  max: 0.400 (0.24s)\n",
      "Gen 266 :   avg: 0.245  -  max: 0.400 (0.24s)\n",
      "Gen 267 :   avg: 0.268  -  max: 0.400 (0.24s)\n",
      "Gen 268 :   avg: 0.287  -  max: 0.400 (0.24s)\n",
      "Gen 269 :   avg: 0.249  -  max: 0.400 (0.22s)\n",
      "Gen 270 :   avg: 0.223  -  max: 0.400 (0.22s)\n",
      "Gen 271 :   avg: 0.279  -  max: 0.400 (0.22s)\n",
      "Gen 272 :   avg: 0.296  -  max: 0.400 (0.22s)\n",
      "Gen 273 :   avg: 0.291  -  max: 0.400 (0.22s)\n",
      "Gen 274 :   avg: 0.242  -  max: 0.400 (0.22s)\n",
      "Gen 275 :   avg: 0.204  -  max: 0.400 (0.22s)\n",
      "Gen 276 :   avg: 0.223  -  max: 0.400 (0.24s)\n",
      "Gen 277 :   avg: 0.211  -  max: 0.400 (0.22s)\n",
      "Gen 278 :   avg: 0.264  -  max: 0.401 (0.22s)\n",
      "Gen 279 :   avg: 0.241  -  max: 0.408 (0.22s)\n",
      "Gen 280 :   avg: 0.236  -  max: 0.408 (0.22s)\n",
      "Gen 281 :   avg: 0.218  -  max: 0.408 (0.22s)\n",
      "Gen 282 :   avg: 0.226  -  max: 0.408 (0.22s)\n",
      "Gen 283 :   avg: 0.259  -  max: 0.408 (0.22s)\n",
      "Gen 284 :   avg: 0.264  -  max: 0.408 (0.24s)\n",
      "Gen 285 :   avg: 0.274  -  max: 0.408 (0.24s)\n",
      "Gen 286 :   avg: 0.249  -  max: 0.408 (0.24s)\n",
      "Gen 287 :   avg: 0.226  -  max: 0.408 (0.24s)\n",
      "Gen 288 :   avg: 0.224  -  max: 0.409 (0.24s)\n",
      "Gen 289 :   avg: 0.292  -  max: 0.409 (0.24s)\n",
      "Gen 290 :   avg: 0.285  -  max: 0.409 (0.24s)\n",
      "Gen 291 :   avg: 0.281  -  max: 0.409 (0.25s)\n",
      "Gen 292 :   avg: 0.261  -  max: 0.409 (0.25s)\n",
      "Gen 293 :   avg: 0.216  -  max: 0.409 (0.26s)\n",
      "Gen 294 :   avg: 0.222  -  max: 0.409 (0.24s)\n",
      "Gen 295 :   avg: 0.244  -  max: 0.409 (0.24s)\n",
      "Gen 296 :   avg: 0.299  -  max: 0.420 (0.25s)\n",
      "Gen 297 :   avg: 0.256  -  max: 0.420 (0.26s)\n",
      "Gen 298 :   avg: 0.269  -  max: 0.420 (0.25s)\n",
      "Gen 299 :   avg: 0.245  -  max: 0.420 (0.24s)\n",
      "Gen 300 :   avg: 0.306  -  max: 0.420 (0.24s)\n",
      "Gen 301 :   avg: 0.270  -  max: 0.420 (0.23s)\n",
      "Gen 302 :   avg: 0.263  -  max: 0.420 (0.25s)\n",
      "Gen 303 :   avg: 0.297  -  max: 0.420 (0.23s)\n",
      "Gen 304 :   avg: 0.275  -  max: 0.420 (0.25s)\n",
      "Gen 305 :   avg: 0.259  -  max: 0.420 (0.24s)\n",
      "Gen 306 :   avg: 0.244  -  max: 0.420 (0.24s)\n",
      "Gen 307 :   avg: 0.282  -  max: 0.420 (0.25s)\n",
      "Gen 308 :   avg: 0.247  -  max: 0.420 (0.22s)\n",
      "Gen 309 :   avg: 0.239  -  max: 0.420 (0.22s)\n",
      "Gen 310 :   avg: 0.232  -  max: 0.420 (0.22s)\n",
      "Gen 311 :   avg: 0.247  -  max: 0.420 (0.22s)\n",
      "Gen 312 :   avg: 0.234  -  max: 0.420 (0.22s)\n",
      "Gen 313 :   avg: 0.266  -  max: 0.420 (0.22s)\n",
      "Gen 314 :   avg: 0.242  -  max: 0.420 (0.22s)\n",
      "Gen 315 :   avg: 0.251  -  max: 0.420 (0.24s)\n",
      "Gen 316 :   avg: 0.216  -  max: 0.420 (0.22s)\n",
      "Gen 317 :   avg: 0.207  -  max: 0.420 (0.22s)\n",
      "Gen 318 :   avg: 0.227  -  max: 0.420 (0.22s)\n",
      "Gen 319 :   avg: 0.203  -  max: 0.420 (0.22s)\n",
      "Gen 320 :   avg: 0.210  -  max: 0.420 (0.22s)\n",
      "Gen 321 :   avg: 0.205  -  max: 0.420 (0.23s)\n",
      "Gen 322 :   avg: 0.208  -  max: 0.420 (0.23s)\n",
      "Gen 323 :   avg: 0.244  -  max: 0.420 (0.23s)\n",
      "Gen 324 :   avg: 0.263  -  max: 0.420 (0.24s)\n",
      "Gen 325 :   avg: 0.247  -  max: 0.420 (0.23s)\n",
      "Gen 326 :   avg: 0.218  -  max: 0.420 (0.24s)\n",
      "Gen 327 :   avg: 0.236  -  max: 0.420 (0.23s)\n",
      "Gen 328 :   avg: 0.251  -  max: 0.420 (0.25s)\n",
      "Gen 329 :   avg: 0.272  -  max: 0.420 (0.22s)\n",
      "Gen 330 :   avg: 0.256  -  max: 0.420 (0.22s)\n",
      "Gen 331 :   avg: 0.258  -  max: 0.420 (0.22s)\n",
      "Gen 332 :   avg: 0.274  -  max: 0.420 (0.22s)\n",
      "Gen 333 :   avg: 0.292  -  max: 0.420 (0.22s)\n",
      "Gen 334 :   avg: 0.250  -  max: 0.423 (0.22s)\n",
      "Gen 335 :   avg: 0.270  -  max: 0.423 (0.22s)\n",
      "Gen 336 :   avg: 0.253  -  max: 0.423 (0.22s)\n",
      "Gen 337 :   avg: 0.277  -  max: 0.424 (0.22s)\n",
      "Gen 338 :   avg: 0.285  -  max: 0.424 (0.22s)\n",
      "Gen 339 :   avg: 0.273  -  max: 0.424 (0.23s)\n",
      "Gen 340 :   avg: 0.249  -  max: 0.424 (0.22s)\n",
      "Gen 341 :   avg: 0.259  -  max: 0.426 (0.22s)\n",
      "Gen 342 :   avg: 0.319  -  max: 0.426 (0.22s)\n",
      "Gen 343 :   avg: 0.312  -  max: 0.426 (0.22s)\n",
      "Gen 344 :   avg: 0.256  -  max: 0.426 (0.22s)\n",
      "Gen 345 :   avg: 0.227  -  max: 0.426 (0.22s)\n",
      "Gen 346 :   avg: 0.203  -  max: 0.426 (0.22s)\n",
      "Gen 347 :   avg: 0.208  -  max: 0.426 (0.22s)\n",
      "Gen 348 :   avg: 0.224  -  max: 0.426 (0.22s)\n",
      "Gen 349 :   avg: 0.224  -  max: 0.426 (0.22s)\n",
      "Gen 350 :   avg: 0.211  -  max: 0.426 (0.22s)\n",
      "Gen 351 :   avg: 0.263  -  max: 0.426 (0.22s)\n",
      "Gen 352 :   avg: 0.247  -  max: 0.426 (0.23s)\n",
      "Gen 353 :   avg: 0.259  -  max: 0.426 (0.23s)\n",
      "Gen 354 :   avg: 0.243  -  max: 0.426 (0.23s)\n",
      "Gen 355 :   avg: 0.269  -  max: 0.426 (0.23s)\n",
      "Gen 356 :   avg: 0.265  -  max: 0.428 (0.22s)\n",
      "Gen 357 :   avg: 0.254  -  max: 0.428 (0.22s)\n",
      "Gen 358 :   avg: 0.276  -  max: 0.428 (0.24s)\n",
      "Gen 359 :   avg: 0.249  -  max: 0.428 (0.24s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:15\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "Input \u001B[1;32mIn [252]\u001B[0m, in \u001B[0;36mPopulation.breed\u001B[1;34m(self, debug)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morganisms \u001B[38;5;241m=\u001B[39m children \u001B[38;5;241m+\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39melite]\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 169\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhistory\u001B[38;5;241m.\u001B[39mappend((\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_fitness\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_fitness()))\n",
      "Input \u001B[1;32mIn [252]\u001B[0m, in \u001B[0;36mPopulation.max_fitness\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmax_fitness\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 50\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morganism_fitness\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Input \u001B[1;32mIn [252]\u001B[0m, in \u001B[0;36mPopulation.organism_fitness\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21morganism_fitness\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness_generation:\n\u001B[1;32m---> 40\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness \u001B[38;5;241m=\u001B[39m [organism\u001B[38;5;241m.\u001B[39mevaluate() \u001B[38;5;28;01mfor\u001B[39;00m organism \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morganisms]\n\u001B[0;32m     41\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness_generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness\n",
      "Input \u001B[1;32mIn [252]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21morganism_fitness\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness_generation:\n\u001B[1;32m---> 40\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness \u001B[38;5;241m=\u001B[39m [\u001B[43morganism\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m organism \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morganisms]\n\u001B[0;32m     41\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness_generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfitness\n",
      "Input \u001B[1;32mIn [251]\u001B[0m, in \u001B[0;36mMultiLayerPerceptron.evaluate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 26\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmean(y_pred \u001B[38;5;241m==\u001B[39m y_true)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36margmax\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TF\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001B[0m, in \u001B[0;36margmax\u001B[1;34m(a, axis, out, keepdims)\u001B[0m\n\u001B[0;32m   1129\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1130\u001B[0m \u001B[38;5;124;03mReturns the indices of the maximum values along an axis.\u001B[39;00m\n\u001B[0;32m   1131\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1213\u001B[0m \u001B[38;5;124;03m(2, 1, 4)\u001B[39;00m\n\u001B[0;32m   1214\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1215\u001B[0m kwds \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeepdims\u001B[39m\u001B[38;5;124m'\u001B[39m: keepdims} \u001B[38;5;28;01mif\u001B[39;00m keepdims \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39m_NoValue \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _wrapfunc(a, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124margmax\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TF\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[1;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[0;32m     52\u001B[0m bound \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, method, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bound \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bound(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\TF\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001B[0m, in \u001B[0;36m_wrapit\u001B[1;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n\u001B[0;32m     42\u001B[0m     wrap \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(asarray(obj), method)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m wrap:\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, mu\u001B[38;5;241m.\u001B[39mndarray):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initial population\n",
    "print('Starting training')\n",
    "t_training = time.time()\n",
    "population = Population(size=POPULATION_SIZE, n_survivors=SURVIVORS, n_hidden_layers=N_HIDDEN_LAYERS)\n",
    "avg_population_fitness = population.average_fitness()\n",
    "max_fitness = population.max_fitness()\n",
    "t2 = time.time()\n",
    "print('Gen {} {:<3} avg: {:.3f} {:^3} max: {:.3f} ({:<3}s)'.format(\n",
    "    0, ':', round(avg_population_fitness, 3), '-', round(max_fitness, 3), round(t2 - t_training, 2)))\n",
    "\n",
    "# future populations\n",
    "for generation in range(1, GENERATIONS):\n",
    "    # breed new population\n",
    "    t1 = time.time()\n",
    "    population.breed()\n",
    "\n",
    "    # evaluate new population\n",
    "    avg_population_fitness = population.average_fitness()\n",
    "    max_fitness = population.max_fitness()\n",
    "    t2 = time.time()\n",
    "\n",
    "    print('Gen {} {:<3} avg: {:.3f} {:^3} max: {:.3f} ({:<3}s)'.format(\n",
    "        generation, ':', round(avg_population_fitness, 3), '-', round(max_fitness, 3), round(t2 - t1, 2)))\n",
    "\n",
    "print('Finished training ({})'.format(round(time.time() - t_training, 2)))\n",
    "\n",
    "# performance of population\n",
    "population.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}